{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'NanumGothicCoding'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import mglearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "\n",
    "import pydotplus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/haeyang/수질.수거량.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop('년월',axis=1)\n",
    "data = df.drop('생태구역',axis=1)\n",
    "data = df.drop('연안명칭',axis=1)\n",
    "data = df.drop('St.\\nNo\\n',axis=1)\n",
    "data = df.drop('구군',axis=1)\n",
    "data = df.drop('수거량',axis=1)\n",
    "data = df.drop('Ds',axis=1)\n",
    "data = df.drop('표층투명도',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,5:35]\n",
    "y = data.iloc[:,4]\n",
    "Y = pd.get_dummies(y).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델생성\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(30,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습\n",
    "hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/deeplearning/mnist_mlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델평가\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('정확도={:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델예측 : 오분류보고서\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "y_test_class = np.argmax(y_test, axis=1)\n",
    "print(y_test_class)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_class)\n",
    "\n",
    "print(classification_report(y_test_class, y_pred_class))\n",
    "print(confusion_matrix(y_test_class, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4. 예측\n",
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.evaluate(X_test, y_test, batch_size=100)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(data.corr(), linewidths=0.1, square=True, annot=True, cmap=plt.cm.viridis, linecolor='black')\n",
    "plt.title('해양환경측정망 상관관계도')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train 원본\n",
    "\n",
    "컬럼 ->  어떤 컬럼을 쓰는가? -> input_data 어떤 넣을것인가?\n",
    "입력방식 -> 1. 직접쓴다. 2. csv파일을 만들어서 돌린다. 3. ???????????????\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터변환\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(\"변환후 데이터크기: {}\".format(X_train_scaled.shape))\n",
    "print(\"스케일 조정전 특성별 최소값: {}\".format(X_train.min(axis=0)))\n",
    "print(\"스케일 조정전 특성별 최대값: {}\".format(X_train.max(axis=0)))\n",
    "print(\"스케일 조정후 특성별 최소값: {}\".format(X_train_scaled.min(axis=0)))\n",
    "print(\"스케일 조정후 특성별 최대값: {}\".format(X_train_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"스케일 조정후 특성별 최소값: {}\".format(X_test_scaled.min(axis=0)))\n",
    "print(\"스케일 조정후 특성별 최대값: {}\".format(X_test_scaled.max(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "fig, axes = plt.subplots(1,3, figsize=(13,4))\n",
    "\n",
    "axes[0].scatter(X_train_scaled[:,0], X_train_scaled[:,1], c=mglearn.cm2(0), \n",
    "                label=\"훈련용\", s=60)\n",
    "axes[0].scatter(X_test_scaled[:,0], X_test_scaled[:,1], c=mglearn.cm2(1), label=\"검증용\", \n",
    "                s=100, marker='^')\n",
    "axes[0].set_title(\"스케일조정안한 데이터\")\n",
    "axes[0].legend(loc=\"upper left\")\n",
    "axes[0].set_title(\"원본데이터\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[1].scatter(X_train_scaled[:,0], X_train_scaled[:,1], c=mglearn.cm2(0), \n",
    "                label=\"훈련용\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:,0], X_test_scaled[:,1], c=mglearn.cm2(1), label=\"검증용\", \n",
    "                s=100, marker='^')\n",
    "axes[1].set_title(\"스케일조정후 데이터\")\n",
    "\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "axes[2].scatter(X_train_scaled[:,0], X_train_scaled[:,1], c=mglearn.cm2(0), \n",
    "                label=\"훈련용\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:,0], X_test_scaled_badly[:,1], c=mglearn.cm2(1), \n",
    "                label=\"검증용\", s=100, marker='^')\n",
    "axes[2].set_title(\"잘못된 스케일조정후 데이터\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"특성0\")\n",
    "    ax.set_ylabel(\"특성1\")\n",
    "fig.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB SCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5)\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "clusters = dbscan.fit_predict(X)\n",
    "plt.scatter(X_scaled[:,0], X_scaled[:,1], c=clusters, cmap=mglearn.cm2, s=60, edgecolors='k')\n",
    "plt.xlabel(\"1st특성\")\n",
    "plt.ylabel(\"2nd특성\")\n",
    "plt.title(\"기본값 eps=0.5를 설정한 DBSCAN학습의 결과 그래프\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# 그림객체 생성\n",
    "fig, axes = plt.subplots(1,4, figsize=(15,3), subplot_kw={'xticks':(), 'yticks':()})\n",
    "\n",
    "# 사용할 머신러닝알고리즘 모델을 리스트로 생성\n",
    "algorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2), DBSCAN()]\n",
    "\n",
    "# 각 알고리즘의 비교를 위해 무작위로 클러스터 할당을 한다.\n",
    "random_state = np.random.RandomState(seed=0)\n",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n",
    "\n",
    "# 무작위로 할당한 클러스터를 시각화\n",
    "axes[0].scatter(X_scaled[:,0], X_scaled[:,1], c=random_clusters, cmap=mglearn.cm3, s=60, edgecolor='k')\n",
    "axes[0].set_title(\"무작위할당-ARI:{:.2f}\".format(adjusted_rand_score(y, random_clusters)))\n",
    "\n",
    "for ax, algorithm in zip(axes[1:], algorithms):\n",
    "    clusters = algorithm.fit_predict(X_scaled)\n",
    "    ax.scatter(X_scaled[:,0], X_scaled[:,1], c=clusters, cmap=mglearn.cm3, s=60, edgecolor='k')\n",
    "    ax.set_title(\"{}-ARI:{:.2f}\".format(algorithm.__class__.__name__,\n",
    "                                        adjusted_rand_score(y, clusters)))\n",
    "plt.show()\n",
    "\n",
    "# 결과분석\n",
    "# 클러스터를 무작위로 할당했을 때의 ARI점수는 0이고, DBSCAN은 점수가 1(완변한 군집)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형회귀 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,5:35]\n",
    "y = data.iloc[:,4]\n",
    "\n",
    "# 훈련용과 검증용데이터를 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# 머신러닝\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print('lr.coef_: {}'.format(lr.coef_))\n",
    "print('lr.intercept_: {}'.format(lr.intercept_))\n",
    "\n",
    "# 기울기 파리미터(w)는 가중치 weight 또는 계수 coefficience라고 하며 coef_속성에 저장되고\n",
    "# 편향 offset 또는 절편 intercept파라미터(b, bias)는 intercept_속성에 저장되어 있다.\n",
    "# intercept속성은 항상 하나의 실수값이지만 coef_속성은 입력특성(값)에 하나씩 대응되는\n",
    "# numpy배열이다. wave데이터셋은 입력특성이 하나이기 때문에 coef_속성도 한개의 값만 가지고 있다.\n",
    "\n",
    "# 뒤의 밑줄(언더바)은 scikit_learn에서 훈련데이터에서 유도된 속성으로서 항상 끝에 밑줄을\n",
    "# 붙이는데 그 이유는 사용자가 지정한 매개변수와 구분하기 위해서 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과분석\n",
    "print(\"훈련세트 점수: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"검증세트 점수: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "# R2값이 0.66인 것은 그리 좋지 않은 결과이다. 하지만 훈련용과 매우 비슷한 것을 알 수 있다.\n",
    "# 그래서 이 모형은 과소적합상태를 의미한다.}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 릿지회귀분석 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,5:35]\n",
    "y = data.iloc[:,4]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "print(\"훈련세트 점수: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"검증세트 점수: {:.2f}\".format(ridge.score(X_test, y_test)))\n",
    "print(ridge)\n",
    "\n",
    "# 결과분석\n",
    "# 훈련세트에서 LR보다는 점수가 낮지만 감증세트의 점수는 높다. 즉, 과대적합이 발생할\n",
    "# 가능성이 적어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 알파값(Reqularization Strength: 과대적합이 되지 않도록 모델을 강제로 제한한다는 의미)을\n",
    "# 조정해서 테스트(알파값을 처음에는 10으로 실행하고 0.1일때랑 비교분석)\n",
    "# 릿지회귀에서 알파값의 기본값은 1.0\n",
    "# 알파값을 높이면 예측율이 낮아지고 낮추면 예측율이 올라간다. \n",
    "# 즉, 알파값을 줄이면 제약이 풀린다.\n",
    "\n",
    "ridge10 = Ridge(alpha=5).fit(X_train, y_train)\n",
    "print(\"훈련세트 점수: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"검증세트 점수: {:.2f}\".format(ridge10.score(X_test, y_test)))\n",
    "print()\n",
    "\n",
    "\n",
    "ridge05 = Ridge(alpha=5).fit(X_train, y_train)\n",
    "print(\"훈련세트 점수: {:.2f}\".format(ridge05.score(X_train, y_train)))\n",
    "print(\"검증세트 점수: {:.2f}\".format(ridge05.score(X_test, y_test)))\n",
    "print()\n",
    "\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"훈련세트 점수: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"검증세트 점수: {:.2f}\".format(ridge01.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef_의 값을 기준으로 시각화\n",
    "plt.plot(ridge10.coef_, '^', label='릿지회귀분석, alpha=10')\n",
    "plt.plot(ridge.coef_, 's', label='릿지회귀분석, alpha=1.0')\n",
    "plt.plot(ridge01.coef_, 'v', label='릿지회귀분석, alpha=0.1')\n",
    "plt.plot(lr.coef_, 'o', label='선형회귀분석')\n",
    "\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 결과분석\n",
    "# x축은 coef_의 값을 위치대로 나열한 것 즉, x=0은 첫번째특성에 연관된 계수이고\n",
    "# x=104는 104번째 특성에 연관된 계수이다.\n",
    "# y축은 각 계수의 수치를 나타낸다. alpha=10일때 -3~3사이에 위치하고\n",
    "# alpha=1일때 모델의 계수는 좀더 커졌고 0.1일 경우일 때눈 더 커졌다.\n",
    "# 아무런 제약이 없는 alpha=0일 때는 그림 범위밖으로 넘어갈 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의사결정나무 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,5:35]\n",
    "y = data.iloc[:,4]\n",
    "print(X[0:5])\n",
    "print(y[0:5])\n",
    "\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "# 표준화데이터셋 생성\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "print(X_train_std[:5])\n",
    "print(X_test_std[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트리모델생성\n",
    "# https://ratsgo.github.io/machine%20learning/2017/03/26/tree/\n",
    "# criterion : 사용할 알고리즘을 정의 gini 또는 entorpy\n",
    "# max_depth = 3\n",
    "data_tree = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n",
    "data_tree.fit(X_train, y_train)\n",
    "print('훈련세트점수: {:.3f}'.format(data_tree.score(X_train, y_train)))\n",
    "print('검증세트점수: {:.3f}'.format(data_tree.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tree.fit(X_train_std, y_train)\n",
    "print('훈련세트점수: {:.3f}'.format(data_tree.score(X_train_std, y_train)))\n",
    "print('검증세트점수: {:.3f}'.format(data_tree.score(X_test_std, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(data_tree, out_file=None, \n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,5:35]\n",
    "y = data.iloc[:,4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2).fit(X_train, y_train)\n",
    "print('훈련세트점수: {:.3f}'.format(forest.score(X_train, y_train)))\n",
    "print('검증세트점수: {:.3f}'.format(forest.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성중요도 시각화\n",
    "def plot_feature_importances_cancer(model):\n",
    "    n_features = X.shape[1]\n",
    "    # print(n_features)\n",
    "    plt.barh(range(n_features), model.feature_importances_, align=\"center\")\n",
    "    plt.yticks(np.arange(n_features),X)\n",
    "    plt.xlabel(\"특성중요도\")\n",
    "    plt.ylabel(\"특성\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.show()\n",
    "    \n",
    "plot_feature_importances_cancer(forest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
